"""
Integrated AI Service (í†µí•© AI ì„œë¹„ìŠ¤)

Gemini 2.0 Flash Thinking / DeepSeek-V3 API + ë¹„ìš© ìµœì í™” í†µí•© ì„œë¹„ìŠ¤
- Prompt Caching (90% ë¹„ìš© ì ˆê°)
- Response Caching (ì¤‘ë³µ í˜¸ì¶œ ì œê±°)
- Smart Sampling (API í˜¸ì¶œ 50~70% ê°ì†Œ)
- Cost Tracking (ì‹¤ì‹œê°„ ë¹„ìš© ëª¨ë‹ˆí„°ë§)
"""

import logging
import hashlib
import json
import requests
from typing import Dict, Any, Optional, List
from datetime import datetime

from .prompt_cache import PromptCacheManager
from .response_cache import ResponseCacheManager
from .smart_sampling import SamplingStrategy, get_global_sampling_manager
from .cost_tracker import CostTracker
from .event_driven_optimizer import EventDrivenOptimizer, MarketEvent, EventType, EventPriority
from src.config import settings
import threading

logger = logging.getLogger(__name__)

# ê¸€ë¡œë²Œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (Issue #4: AI Rate Limit ë¬¸ì œ í•´ê²°)
_integrated_ai_service_instance: Optional["IntegratedAIService"] = None
_service_lock = threading.Lock()


def get_integrated_ai_service(redis_client=None) -> "IntegratedAIService":
    """
    ê¸€ë¡œë²Œ IntegratedAIService ì‹±ê¸€í†¤ ë°˜í™˜

    Thread-safe singleton patternì„ ì‚¬ìš©í•˜ì—¬ ì „ì—­ì—ì„œ í•˜ë‚˜ì˜
    IntegratedAIService ì¸ìŠ¤í„´ìŠ¤ë§Œ ìƒì„±ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

    Issue #4: AI Rate Limit ë¬¸ì œ í•´ê²°
    - ê¸°ì¡´: ì „ëµ ì¸ìŠ¤í„´ìŠ¤ë§ˆë‹¤ ìƒˆ IntegratedAIService ìƒì„±
    - ìˆ˜ì •: ê¸€ë¡œë²Œ ì‹±ê¸€í†¤ìœ¼ë¡œ API í‚¤, Rate Limit ê´€ë¦¬ ì¼ì›í™”

    Args:
        redis_client: Redis í´ë¼ì´ì–¸íŠ¸ (ì²« í˜¸ì¶œ ì‹œë§Œ ì‚¬ìš©)

    Returns:
        IntegratedAIService: ê¸€ë¡œë²Œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
    """
    global _integrated_ai_service_instance

    # Double-checked locking for thread safety
    if _integrated_ai_service_instance is None:
        with _service_lock:
            if _integrated_ai_service_instance is None:
                _integrated_ai_service_instance = IntegratedAIService(redis_client)
                logger.info("âœ… Global IntegratedAIService singleton initialized")

    return _integrated_ai_service_instance


class IntegratedAIService:
    """
    í†µí•© AI ì„œë¹„ìŠ¤

    Gemini 2.0 Flash Thinking / DeepSeek-V3 + 4ê°€ì§€ ë¹„ìš© ìµœì í™” ê¸°ëŠ¥ í†µí•©:
    1. Prompt Caching: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìºì‹± (90% í• ì¸)
    2. Response Caching: ë™ì¼ ì¿¼ë¦¬ ì‘ë‹µ ì¬ì‚¬ìš©
    3. Smart Sampling: ì§€ëŠ¥ì  API í˜¸ì¶œ ìƒ˜í”Œë§
    4. Cost Tracking: ì‹¤ì‹œê°„ ë¹„ìš© ì¶”ì 

    ë¹„ìš© ì ˆê° íš¨ê³¼:
    - ê¸°ì¡´: $1,000/month
    - ìµœì í™” í›„: $300/month (70% ì ˆê°)
    """

    # Gemini ëª¨ë¸ ì„¤ì • (2025.12 ê¸°ì¤€ ìµœì‹ )
    GEMINI_MODEL = "gemini-3-pro-preview"  # Gemini 3 Pro Preview (Deep Think)
    GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta"

    # DeepSeek V3.2 Release ëª¨ë¸ (í´ë°±ìš©)
    DEEPSEEK_MODEL = "deepseek-chat"
    DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"

    def __init__(self, redis_client=None):
        # AI Provider ì„ íƒ (ê¸°ë³¸ê°’: gemini)
        self.ai_provider = settings.ai_provider.lower()

        if self.ai_provider == "gemini":
            self.api_key = settings.gemini_api_key
            if not self.api_key or self.api_key == "":
                raise ValueError("Gemini API key not configured. Set GEMINI_API_KEY environment variable.")
            masked_key = f"{'*' * 8}{self.api_key[-4:]}" if len(self.api_key) > 4 else "****"
            logger.info(f"ğŸŒŸ Gemini API key configured: {masked_key}")
            logger.info(f"ğŸ§  Using Gemini 2.0 Flash Thinking for advanced market analysis")
        else:
            self.api_key = settings.deepseek_api_key
            if not self.api_key or self.api_key == "":
                raise ValueError("DeepSeek API key not configured. Set DEEPSEEK_API_KEY environment variable.")
            masked_key = f"{'*' * 8}{self.api_key[-4:]}" if len(self.api_key) > 4 else "****"
            logger.info(f"DeepSeek API key configured: {masked_key}")

        self.redis_client = redis_client

        # ë¹„ìš© ìµœì í™” ë§¤ë‹ˆì €ë“¤
        self.prompt_cache = PromptCacheManager(redis_client)
        self.response_cache = ResponseCacheManager(redis_client)
        # Issue #4: ê¸€ë¡œë²Œ ì‹±ê¸€í†¤ SmartSamplingManager ì‚¬ìš© (ìºì‹œ ìƒíƒœ ìœ ì§€)
        self.sampling_manager = get_global_sampling_manager()
        self.cost_tracker = CostTracker(redis_client)
        self.event_optimizer = EventDrivenOptimizer(redis_client)

        provider_name = "Gemini 2.0 Flash Thinking" if self.ai_provider == "gemini" else "DeepSeek-V3"
        logger.info(f"IntegratedAIService initialized with {provider_name} + event-driven cost optimization")

    async def call_ai(
        self,
        agent_type: str,
        prompt: str,
        context: Dict[str, Any],
        system_prompt: Optional[str] = None,
        response_type: str = "general",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        enable_caching: bool = True,
        enable_sampling: bool = True,
    ) -> Dict[str, Any]:
        """
        AI API í˜¸ì¶œ (ë¹„ìš© ìµœì í™” ì ìš©)

        Args:
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì… (market_regime, signal_validator, etc.)
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            context: ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° (ìƒ˜í”Œë§/ìºì‹±ì— ì‚¬ìš©)
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìºì‹± ëŒ€ìƒ)
            response_type: ì‘ë‹µ íƒ€ì… (ìºì‹± TTL ê²°ì •)
            temperature: ì˜¨ë„ ì„¤ì •
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            enable_caching: ì‘ë‹µ ìºì‹± í™œì„±í™”
            enable_sampling: ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ í™œì„±í™”

        Returns:
            {
                "response": str,
                "cost_info": {...},
                "cache_hit": bool,
                "sampled": bool
            }
        """
        # 1. ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì²´í¬
        sampled = True
        skip_reason = None

        if enable_sampling:
            should_sample, reason = await self.sampling_manager.should_sample(
                agent_type=agent_type,
                context=context
            )

            if not should_sample:
                logger.info(f"â­ï¸  Skipping AI call for {agent_type}: {reason}")

                # ì´ì „ ì‘ë‹µ ì¬ì‚¬ìš© (ìºì‹œì—ì„œ)
                cached_response = await self.response_cache.get_cached_response(
                    response_type=response_type,
                    query_data={"prompt": prompt, "context": context}
                )

                if cached_response:
                    return {
                        "response": cached_response.get("response", ""),
                        "cost_info": {"cost_usd": 0.0},
                        "cache_hit": True,
                        "sampled": False,
                        "skip_reason": reason,
                    }

                # ìºì‹œë„ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ
                return {
                    "response": self._get_default_response(agent_type),
                    "cost_info": {"cost_usd": 0.0},
                    "cache_hit": False,
                    "sampled": False,
                    "skip_reason": reason,
                }

        # 2. ì‘ë‹µ ìºì‹œ ì¡°íšŒ
        cache_hit = False

        if enable_caching and self.response_cache.should_cache(response_type, context):
            cached_response = await self.response_cache.get_cached_response(
                response_type=response_type,
                query_data={"prompt": prompt, "context": context}
            )

            if cached_response:
                logger.info(f"âœ… Response cache HIT for {agent_type}")
                cache_hit = True

                return {
                    "response": cached_response.get("response", ""),
                    "cost_info": {"cost_usd": 0.0},
                    "cache_hit": True,
                    "sampled": True,
                }

        # 3. í”„ë¡¬í”„íŠ¸ ìºì‹œ ì¡°íšŒ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
        if system_prompt and enable_caching:
            cached_system = await self.prompt_cache.get_cached_prompt(
                prompt_type="agent_prompt",
                context_data={"agent_type": agent_type}
            )

            if cached_system:
                system_prompt = cached_system
                logger.debug(f"Prompt cache HIT for {agent_type}")

        # 4. AI API í˜¸ì¶œ (Gemini or DeepSeek)
        try:
            if self.ai_provider == "gemini":
                response_text, usage = await self._call_gemini_api(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                model_name = "gemini-2.5-pro"
            else:
                response_text, usage = await self._call_deepseek_api(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                model_name = "deepseek-v3"

            # 5. ë¹„ìš© ì¶”ì 
            cost_info = await self.cost_tracker.track_api_call(
                model=model_name,
                agent_type=agent_type,
                input_tokens=usage.get("prompt_tokens", 0),
                output_tokens=usage.get("completion_tokens", 0),
                cache_read_tokens=usage.get("cache_read_tokens", 0),
                cache_write_tokens=usage.get("cache_write_tokens", 0),
                metadata={"response_type": response_type}
            )

            # 6. ì‘ë‹µ ìºì‹±
            if enable_caching:
                await self.response_cache.set_cached_response(
                    response_type=response_type,
                    query_data={"prompt": prompt, "context": context},
                    response={"response": response_text, "cost_info": cost_info}
                )

            # 7. í”„ë¡¬í”„íŠ¸ ìºì‹± (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
            if system_prompt and enable_caching:
                await self.prompt_cache.set_cached_prompt(
                    prompt_type="agent_prompt",
                    context_data={"agent_type": agent_type},
                    prompt=system_prompt
                )

            logger.info(
                f"âœ… AI call for {agent_type}: ${cost_info['total_cost_usd']:.6f}, "
                f"{usage.get('total_tokens', 0)} tokens"
            )

            # ì„±ê³µ ì‹œ Rate Limit backoff ê°ì†Œ
            self.sampling_manager.notify_success()

            return {
                "response": response_text,
                "cost_info": cost_info,
                "cache_hit": False,
                "sampled": True,
            }

        except Exception as e:
            error_str = str(e)
            logger.error(f"AI API call failed for {agent_type}: {e}", exc_info=True)

            # Rate Limit (429) ì—ëŸ¬ ê°ì§€ ë° backoff ì ìš©
            if "429" in error_str or "Too Many Requests" in error_str or "rate limit" in error_str.lower():
                self.sampling_manager.notify_rate_limit()
                logger.warning(
                    f"ğŸš¨ Rate limit detected for {agent_type}, "
                    f"backoff status: {self.sampling_manager.get_backoff_status()}"
                )

            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            return {
                "response": self._get_default_response(agent_type),
                "cost_info": {"cost_usd": 0.0},
                "cache_hit": False,
                "sampled": True,
                "error": error_str,
            }

    async def call_ai_with_event(
        self,
        event: MarketEvent,
        agent_type: str,
        prompt: str,
        context: Dict[str, Any],
        system_prompt: Optional[str] = None,
        response_type: str = "general",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        enable_caching: bool = True,
        enable_sampling: bool = True,
    ) -> Dict[str, Any]:
        """
        ì´ë²¤íŠ¸ ê¸°ë°˜ AI í˜¸ì¶œ (ë¹„ìš© ìµœì í™”)

        Args:
            event: ì‹œì¥ ì´ë²¤íŠ¸
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì…
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            context: ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            response_type: ì‘ë‹µ íƒ€ì…
            temperature: ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í°
            enable_caching: ìºì‹± í™œì„±í™”
            enable_sampling: ìƒ˜í”Œë§ í™œì„±í™”

        Returns:
            AI ì‘ë‹µ ê²°ê³¼
        """
        # 1. ì´ë²¤íŠ¸ ê¸°ë°˜ í•„í„°ë§
        should_call, reason = self.event_optimizer.should_trigger_ai(
            event=event,
            agent_type=agent_type
        )

        if not should_call:
            logger.info(
                f"â­ï¸  Event filtered: {event.event_type.value} for {event.symbol} -> {reason}"
            )

            # ìºì‹œëœ ì‘ë‹µ ì¬ì‚¬ìš©
            cached_response = await self.response_cache.get_cached_response(
                response_type=response_type,
                query_data={"prompt": prompt, "context": context}
            )

            if cached_response:
                return {
                    "response": cached_response.get("response", ""),
                    "cost_info": {"cost_usd": 0.0},
                    "cache_hit": True,
                    "sampled": False,
                    "event_filtered": True,
                    "filter_reason": reason,
                }

            # ê¸°ë³¸ ì‘ë‹µ
            return {
                "response": self._get_default_response(agent_type),
                "cost_info": {"cost_usd": 0.0},
                "cache_hit": False,
                "sampled": False,
                "event_filtered": True,
                "filter_reason": reason,
            }

        # 2. ë°°ì¹˜ ì²˜ë¦¬ ì²´í¬ (LOW ìš°ì„ ìˆœìœ„ ì´ë²¤íŠ¸)
        if event.priority == EventPriority.LOW:
            batch = self.event_optimizer.get_batch_if_ready(event.symbol)

            if batch:
                logger.info(
                    f"ğŸ“¦ Processing batch of {len(batch)} events for {event.symbol}"
                )
                # ë°°ì¹˜ ì²˜ë¦¬ (ì—¬ëŸ¬ ì´ë²¤íŠ¸ë¥¼ í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ)
                return await self._process_event_batch(
                    batch=batch,
                    agent_type=agent_type,
                    system_prompt=system_prompt,
                    response_type=response_type,
                    temperature=temperature,
                    max_tokens=max_tokens
                )

            # ë°°ì¹˜ ëŒ€ê¸° ì¤‘
            return {
                "response": self._get_default_response(agent_type),
                "cost_info": {"cost_usd": 0.0},
                "cache_hit": False,
                "sampled": False,
                "batched": True,
            }

        # 3. AI í˜¸ì¶œ (ì´ë²¤íŠ¸ í†µê³¼)
        result = await self.call_ai(
            agent_type=agent_type,
            prompt=prompt,
            context=context,
            system_prompt=system_prompt,
            response_type=response_type,
            temperature=temperature,
            max_tokens=max_tokens,
            enable_caching=enable_caching,
            enable_sampling=enable_sampling
        )

        # AI í˜¸ì¶œ ì‹œê°„ ê¸°ë¡
        self.event_optimizer.mark_ai_called(event.symbol)

        return result

    async def _process_event_batch(
        self,
        batch: List[MarketEvent],
        agent_type: str,
        system_prompt: Optional[str],
        response_type: str,
        temperature: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """
        ì´ë²¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ (í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ì—¬ëŸ¬ ì´ë²¤íŠ¸ ì²˜ë¦¬)

        Args:
            batch: ì´ë²¤íŠ¸ ë°°ì¹˜
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì…
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            response_type: ì‘ë‹µ íƒ€ì…
            temperature: ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í°

        Returns:
            AI ì‘ë‹µ ê²°ê³¼
        """
        # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        batch_summary = "\n".join([
            f"- Event {i+1}: {e.event_type.value} at {e.timestamp.strftime('%H:%M:%S')}, "
            f"Data: {e.data}"
            for i, e in enumerate(batch)
        ])

        prompt = f"""Analyze the following batch of market events for {batch[0].symbol}:

{batch_summary}

Provide a comprehensive analysis of these events and their combined implications."""

        context = {
            "symbol": batch[0].symbol,
            "batch_size": len(batch),
            "event_types": [e.event_type.value for e in batch]
        }

        # AI í˜¸ì¶œ
        result = await self.call_ai(
            agent_type=agent_type,
            prompt=prompt,
            context=context,
            system_prompt=system_prompt,
            response_type=response_type,
            temperature=temperature,
            max_tokens=max_tokens,
            enable_caching=True,
            enable_sampling=False  # ë°°ì¹˜ëŠ” ìƒ˜í”Œë§ ìŠ¤í‚µ
        )

        result["batched"] = True
        result["batch_size"] = len(batch)

        logger.info(f"âœ… Batch processed: {len(batch)} events, ${result['cost_info']['total_cost_usd']:.6f}")

        return result

    async def _call_gemini_api(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> tuple[str, Dict[str, int]]:
        """
        Gemini 3 Pro (Deep Think) API í˜¸ì¶œ

        Returns:
            (response_text, usage_dict)
        """
        if not self.api_key:
            raise ValueError("Gemini API key is not configured")

        url = f"{self.GEMINI_BASE_URL}/models/{self.GEMINI_MODEL}:generateContent?key={self.api_key}"

        headers = {
            "Content-Type": "application/json",
        }

        # Gemini í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ êµ¬ì„±
        contents = []

        if system_prompt:
            contents.append({
                "role": "user",
                "parts": [{"text": f"System Instructions: {system_prompt}"}]
            })
            contents.append({
                "role": "model",
                "parts": [{"text": "I understand. I will follow these instructions."}]
            })

        contents.append({
            "role": "user",
            "parts": [{"text": prompt}]
        })

        payload = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
                "topP": 0.95,
                "topK": 40,
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
        }

        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=60,  # Gemini Deep ThinkëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒ í•„ìš”
            )
            response.raise_for_status()

            data = response.json()

            # ì‘ë‹µ ì¶”ì¶œ
            response_text = ""
            if "candidates" in data and len(data["candidates"]) > 0:
                candidate = data["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    response_text = candidate["content"]["parts"][0].get("text", "")

            # ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (Gemini í˜•ì‹)
            usage_metadata = data.get("usageMetadata", {})
            usage = {
                "prompt_tokens": usage_metadata.get("promptTokenCount", 0),
                "completion_tokens": usage_metadata.get("candidatesTokenCount", 0),
                "total_tokens": usage_metadata.get("totalTokenCount", 0),
            }

            return response_text, usage

        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            raise

    async def _call_deepseek_api(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> tuple[str, Dict[str, int]]:
        """
        DeepSeek API í˜¸ì¶œ (í´ë°±ìš©)

        Returns:
            (response_text, usage_dict)
        """
        if not self.api_key:
            raise ValueError("DeepSeek API key is not configured")

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        messages = []

        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt,
            })

        messages.append({
            "role": "user",
            "content": prompt,
        })

        payload = {
            "model": self.DEEPSEEK_MODEL,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        try:
            response = requests.post(
                f"{self.DEEPSEEK_BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
                timeout=30,
            )
            response.raise_for_status()

            data = response.json()

            # ì‘ë‹µ ì¶”ì¶œ
            response_text = ""
            if "choices" in data and len(data["choices"]) > 0:
                response_text = data["choices"][0]["message"]["content"]

            # ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            usage = data.get("usage", {})

            return response_text, usage

        except Exception as e:
            logger.error(f"DeepSeek API error: {e}")
            raise

    def _get_default_response(self, agent_type: str) -> str:
        """ê¸°ë³¸ ì‘ë‹µ (ì—ëŸ¬ ì‹œ ë˜ëŠ” ìƒ˜í”Œë§ ìŠ¤í‚µ ì‹œ)"""
        defaults = {
            "market_regime": "UNKNOWN",
            "signal_validator": "HOLD",
            "anomaly_detector": "NO_ANOMALY",
            "portfolio_optimizer": "NO_REBALANCING",
            "risk_monitor": "NORMAL",
        }

        return defaults.get(agent_type, "NO_ACTION")

    async def get_cost_stats(self) -> Dict[str, Any]:
        """ë¹„ìš© í†µê³„ ì¡°íšŒ"""
        return {
            "overall": self.cost_tracker.get_overall_stats(),
            "prompt_cache": self.prompt_cache.get_cache_stats(),
            "response_cache": self.response_cache.get_cache_stats(),
            "sampling": self.sampling_manager.get_sampling_stats(),
        }

    async def get_daily_cost(self) -> Dict[str, Any]:
        """ì˜¤ëŠ˜ ë¹„ìš© ì¡°íšŒ"""
        return await self.cost_tracker.get_daily_cost()

    async def get_monthly_cost(self) -> Dict[str, Any]:
        """ì´ë²ˆ ë‹¬ ë¹„ìš© ì¡°íšŒ"""
        return await self.cost_tracker.get_monthly_cost()

    async def check_budget_alert(
        self, daily_budget: float = 10.0, monthly_budget: float = 300.0
    ) -> Dict[str, Any]:
        """ì˜ˆì‚° ì•Œë¦¼ ì²´í¬"""
        return await self.cost_tracker.check_budget_alert(
            daily_budget=daily_budget,
            monthly_budget=monthly_budget
        )

    async def get_agent_breakdown(self) -> List[Dict[str, Any]]:
        """ì—ì´ì „íŠ¸ë³„ ë¹„ìš© ë¶„ì„"""
        return await self.cost_tracker.get_agent_breakdown()

    def configure_sampling_strategy(
        self,
        agent_type: str,
        strategy: SamplingStrategy,
        config: Dict[str, Any] = None
    ):
        """ìƒ˜í”Œë§ ì „ëµ ë™ì  ë³€ê²½"""
        self.sampling_manager.override_strategy(
            agent_type=agent_type,
            strategy=strategy,
            config=config
        )

        logger.info(f"Sampling strategy updated: {agent_type} -> {strategy.value}")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (Redis í´ë¼ì´ì–¸íŠ¸ëŠ” ë‚˜ì¤‘ì— ì£¼ì…)
_integrated_ai_service_instance = None


def get_integrated_ai_service(redis_client=None) -> IntegratedAIService:
    """í†µí•© AI ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ"""
    global _integrated_ai_service_instance

    if _integrated_ai_service_instance is None:
        _integrated_ai_service_instance = IntegratedAIService(redis_client)

    return _integrated_ai_service_instance
