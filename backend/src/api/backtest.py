import os
import uuid
from datetime import datetime
from pathlib import Path
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.security import HTTPAuthorizationCredentials
from sqlalchemy.orm import Session

from ..schemas.backtest_schema import BacktestStartRequest
from ..schemas.backtest_response_schema import BacktestStartResponse
from ..services.backtest_engine import BacktestEngine
from ..services.backtest_persistence import BacktestPersistenceService
from ..services.strategies.registry import get_strategy
from ..database.session import get_session
from ..database.models import BacktestResult
from ..utils.jwt_auth import get_current_user_id
from ..utils.resource_manager import resource_manager
from ..config import BacktestConfig

router = APIRouter(prefix="/backtest", tags=["backtest"])


def validate_csv_path(csv_path: str) -> None:
    """
    CSV íŒŒì¼ ê²½ë¡œ ê²€ì¦ (Path Traversal ë°©ì§€)

    Args:
        csv_path: ê²€ì¦í•  CSV íŒŒì¼ ê²½ë¡œ

    Raises:
        HTTPException: ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì•ˆì „í•˜ì§€ ì•Šì€ ê²½ìš°

    Security:
        - ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ ".." ë“±ì˜ ìƒëŒ€ ê²½ë¡œ ê³µê²© ë°©ì§€
        - í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        - íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    """
    try:
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        abs_path = Path(csv_path).resolve()

        # í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ëª©ë¡ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
        # ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        project_root = Path(__file__).parent.parent.parent
        allowed_dirs = [
            project_root / "data",
            project_root / "backtest_data",
            project_root / "uploads",
        ]

        # íŒŒì¼ì´ í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        is_allowed = any(
            abs_path.is_relative_to(allowed_dir) for allowed_dir in allowed_dirs
        )

        if not is_allowed:
            raise HTTPException(
                status_code=403,
                detail="Access denied: CSV file must be in an allowed directory",
            )

        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not abs_path.exists():
            raise HTTPException(
                status_code=400, detail=f"CSV file not found: {csv_path}"
            )

        # CSV íŒŒì¼ì¸ì§€ í™•ì¸
        if abs_path.suffix.lower() != ".csv":
            raise HTTPException(status_code=400, detail="File must have .csv extension")

    except ValueError as e:
        # Path.is_relative_to() ì—ëŸ¬ (Python 3.9+)
        raise HTTPException(status_code=403, detail="Invalid file path")


def _run_backtest_background(result_id: int, request_dict: dict, user_id: int):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ë°±í…ŒìŠ¤íŠ¸ ì‘ì—….
    ë³„ë„ DB ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰.
    """
    import logging
    import asyncio
    from datetime import datetime
    from ..utils.resource_manager import resource_manager

    logger = logging.getLogger(__name__)
    logger.info(f"Starting background backtest for result_id={result_id}")

    from ..database.session import _get_sync_engine

    SessionLocal = _get_sync_engine()
    session = SessionLocal()

    try:
        logger.info(f"Running backtest with params: {request_dict}")

        # CSV ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ê³¼ê±° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        csv_path = request_dict.get("csv_path")

        if not csv_path:
            logger.info(
                "CSV path not provided, fetching historical data from cache/API"
            )

            # ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ìš°ì„ , ì—†ìœ¼ë©´ API í˜¸ì¶œ)
            symbol = request_dict.get("symbol", "BTCUSDT")
            # Symbol í˜•ì‹ ë³€í™˜: "BTC/USDT" -> "BTCUSDT"
            symbol = symbol.replace("/", "")
            timeframe = request_dict.get("timeframe", "1h")
            start_date = request_dict.get("start_date")
            end_date = request_dict.get("end_date")

            # ê¸°ë³¸ê°’ ì„¤ì • (ì—†ìœ¼ë©´ ìµœê·¼ 1ë…„)
            from datetime import datetime, timedelta

            if not end_date:
                end_date = datetime.now().strftime("%Y-%m-%d")
            if not start_date:
                start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

            # ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš© (Rate Limit ë¬¸ì œ í•´ê²°!)
            from ..services.candle_cache import get_candle_cache

            async def fetch_historical_data():
                cache_manager = get_candle_cache()
                # í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ (ê¸°ë³¸: ì˜¤í”„ë¼ì¸ ëª¨ë“œ)
                # Rate Limit (429) ì—ëŸ¬ ë°©ì§€
                candles = await cache_manager.get_candles(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_date=start_date,
                    end_date=end_date,
                    cache_only=BacktestConfig.CACHE_ONLY,  # í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´
                )
                return candles

            historical_data = asyncio.run(fetch_historical_data())

            if not historical_data:
                # ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œ ë” ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€
                mode_info = (
                    "ì˜¤í”„ë¼ì¸ ëª¨ë“œ" if BacktestConfig.CACHE_ONLY else "ì˜¨ë¼ì¸ ëª¨ë“œ"
                )
                raise Exception(
                    f"ğŸ“Š {mode_info}: í•´ë‹¹ ê¸°ê°„ì˜ ìº”ë“¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                    f"â€¢ ì‹¬ë³¼: {symbol}\n"
                    f"â€¢ íƒ€ì„í”„ë ˆì„: {timeframe}\n"
                    f"â€¢ ê¸°ê°„: {start_date} ~ {end_date}\n\n"
                    f"ğŸ’¡ í•´ê²° ë°©ë²•:\n"
                    f"1. ë‹¤ë¥¸ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš”\n"
                    f"2. ê´€ë¦¬ìì—ê²Œ ë°ì´í„° ë‹¤ìš´ë¡œë“œë¥¼ ìš”ì²­í•˜ì„¸ìš”\n"
                    f"   (python scripts/download_candle_data.py --symbols {symbol})"
                )

            # CSV íŒŒì¼ë¡œ ì €ì¥
            import csv
            import tempfile
            from pathlib import Path

            # backtest_data ë””ë ‰í† ë¦¬ì— ì €ì¥
            backtest_data_dir = Path(__file__).parent.parent.parent / "backtest_data"
            backtest_data_dir.mkdir(parents=True, exist_ok=True)

            csv_filename = f"backtest_{result_id}_{symbol}_{timeframe}.csv"
            csv_path = str(backtest_data_dir / csv_filename)

            with open(csv_path, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["timestamp", "open", "high", "low", "close", "volume"])

                for candle in historical_data:
                    writer.writerow(
                        [
                            candle["timestamp"],
                            candle["open"],
                            candle["high"],
                            candle["low"],
                            candle["close"],
                            candle["volume"],
                        ]
                    )

            logger.info(
                f"Historical data saved to {csv_path} ({len(historical_data)} candles)"
            )

            # request_dict ì—…ë°ì´íŠ¸
            request_dict["csv_path"] = csv_path

        # ì „ëµ ì„ íƒ
        strategy_code = request_dict.get("strategy_code", "openclose")
        strategy_params = request_dict.get("strategy_params", {})

        strategy = get_strategy(strategy_code, strategy_params)

        # ì—”ì§„ ì‹¤í–‰ (ë¹„ë™ê¸°)
        engine = BacktestEngine(strategy=strategy)
        run_output = asyncio.run(engine.run(request_dict))

        # DB ì—…ë°ì´íŠ¸ - ì„±ê³µ
        result = (
            session.query(BacktestResult).filter(BacktestResult.id == result_id).first()
        )
        if result:
            result.final_balance = run_output.get("final_balance")
            result.equity_curve = str(run_output.get("equity_curve", []))
            result.status = "completed"

            # ê±°ë˜ ë‚´ì—­ ì €ì¥ (ê¸°ì¡´ result ì—…ë°ì´íŠ¸)
            BacktestPersistenceService.save_result(
                session=session,
                run_output=run_output,
                request_params=request_dict,
                result_id=result_id,
            )

            # ê±°ë˜ê°€ ì €ì¥ë˜ë„ë¡ flush (commit ì „ì— DBì— ë°˜ì˜)
            session.flush()

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (CRITICAL: ì „ëµ í‰ê°€ë¥¼ ìœ„í•œ í•µì‹¬ ì§€í‘œ)
            try:
                import json
                from ..database.models import BacktestTrade

                initial_balance = float(request_dict["initial_balance"])
                final_balance = float(result.final_balance)

                # ì´ ìˆ˜ìµë¥ 
                total_return = (
                    (final_balance - initial_balance) / initial_balance
                ) * 100

                # ê±°ë˜ í†µê³„
                all_trades = (
                    session.query(BacktestTrade)
                    .filter(BacktestTrade.result_id == result_id)
                    .all()
                )

                total_trades = len(all_trades)

                if total_trades > 0:
                    # ìŠ¹/íŒ¨ ê±°ë˜ êµ¬ë¶„
                    winning_trades = [t for t in all_trades if t.pnl and t.pnl > 0]
                    losing_trades = [t for t in all_trades if t.pnl and t.pnl < 0]

                    win_count = len(winning_trades)
                    loss_count = len(losing_trades)
                    win_rate = (
                        (win_count / total_trades * 100) if total_trades > 0 else 0
                    )

                    # Profit Factor
                    gross_profit = sum(float(t.pnl) for t in winning_trades)
                    gross_loss = abs(sum(float(t.pnl) for t in losing_trades))
                    profit_factor = (gross_profit / gross_loss) if gross_loss > 0 else 0

                    # í‰ê·  ìŠ¹/íŒ¨
                    avg_win = (gross_profit / win_count) if win_count > 0 else 0
                    avg_loss = (gross_loss / loss_count) if loss_count > 0 else 0
                else:
                    win_rate = 0
                    profit_factor = 0
                    avg_win = 0
                    avg_loss = 0

                # Maximum Drawdown
                equity_curve = (
                    json.loads(result.equity_curve) if result.equity_curve else []
                )
                max_dd = 0
                peak = equity_curve[0] if equity_curve else initial_balance

                for balance in equity_curve:
                    if balance > peak:
                        peak = balance
                    drawdown = ((peak - balance) / peak * 100) if peak > 0 else 0
                    if drawdown > max_dd:
                        max_dd = drawdown

                # Sharpe Ratio (ë‹¨ìˆœí™”ëœ ë²„ì „ - ë¬´ìœ„í—˜ ìˆ˜ìµë¥  0% ê°€ì •)
                if len(equity_curve) > 1:
                    returns = []
                    for i in range(1, len(equity_curve)):
                        ret = (equity_curve[i] - equity_curve[i - 1]) / equity_curve[
                            i - 1
                        ]
                        returns.append(ret)

                    try:
                        import numpy as np

                        mean_return = np.mean(returns)
                        std_return = np.std(returns)
                        sharpe_ratio = (
                            (mean_return / std_return * np.sqrt(252))
                            if std_return > 0
                            else 0
                        )
                    except ImportError:
                        # numpy ì—†ìœ¼ë©´ ìˆ˜ë™ ê³„ì‚°
                        mean_return = sum(returns) / len(returns)
                        variance = sum((r - mean_return) ** 2 for r in returns) / len(
                            returns
                        )
                        std_return = variance**0.5
                        sharpe_ratio = (
                            (mean_return / std_return * (252**0.5))
                            if std_return > 0
                            else 0
                        )
                else:
                    sharpe_ratio = 0

                # ë©”íŠ¸ë¦­ ì»´íŒŒì¼
                metrics = {
                    "total_return": round(total_return, 2),
                    "total_trades": total_trades,
                    "win_rate": round(win_rate, 2),
                    "profit_factor": round(profit_factor, 2),
                    "avg_win": round(avg_win, 2),
                    "avg_loss": round(avg_loss, 2),
                    "sharpe_ratio": round(sharpe_ratio, 2),
                    "max_drawdown": round(max_dd, 2),
                }

                # DBì— ë©”íŠ¸ë¦­ ì €ì¥
                result.metrics = json.dumps(metrics)
                session.commit()

                logger.info(
                    f"âœ… Backtest metrics calculated for result {result_id}: {metrics}"
                )

            except Exception as e:
                logger.error(
                    f"âŒ Failed to calculate metrics for result {result_id}: {e}",
                    exc_info=True,
                )
                # ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨í•´ë„ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ëŠ” ìœ íš¨í•¨
                result.metrics = "{}"
                session.commit()

        session.commit()
        logger.info(f"Backtest completed successfully for result_id={result_id}")

    except Exception as exc:
        logger.error(f"Backtest failed for result_id={result_id}: {exc}", exc_info=True)
        # DB ì—…ë°ì´íŠ¸ - ì‹¤íŒ¨
        try:
            result = (
                session.query(BacktestResult)
                .filter(BacktestResult.id == result_id)
                .first()
            )
            if result:
                result.status = "failed"
                result.error_message = str(exc)
            session.commit()
        except Exception as e:
            logger.error(f"Failed to update error status: {e}")
    finally:
        # ë¦¬ì†ŒìŠ¤ í•´ì œ (Rate Limit í•´ì œ)
        resource_manager.finish_backtest(user_id, result_id)
        session.close()


@router.post("/start", response_model=BacktestStartResponse)
async def start_backtest(
    request: BacktestStartRequest,
    background_tasks: BackgroundTasks,
    session: Session = Depends(get_session),
    user_id: int = Depends(get_current_user_id),
):
    """
    ë°±í…ŒìŠ¤íŠ¸ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ.

    JWT ì¸ì¦ í•„ìš”.
    ì‚¬ìš©ìë³„ ë¦¬ì†ŒìŠ¤ ì œí•œ ì ìš©.

    Parameters:
    - strategy_id: ë°ì´í„°ë² ì´ìŠ¤ì˜ ì „ëµ ID
    - initial_balance: ì‹œì‘ ì”ê³ 
    - start_date: ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
    - end_date: ë°±í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
    - csv_path: (ì˜µì…˜) CSV íŒŒì¼ ê²½ë¡œ

    Returns:
    - result_id: ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ID
    - status: "queued" (ì²˜ë¦¬ ì¤‘)

    ê²°ê³¼ ì¡°íšŒ: GET /backtest/result/{result_id}
    """
    from sqlalchemy import select
    from ..database.models import Strategy

    # 0) ë¦¬ì†ŒìŠ¤ ì œí•œ í™•ì¸
    can_start, error_msg = resource_manager.can_start_backtest(user_id)
    if not can_start:
        raise HTTPException(status_code=429, detail=error_msg)

    # 1) Strategy validation - DBì—ì„œ ì „ëµ ì¡°íšŒ (sync session)
    result = session.execute(select(Strategy).where(Strategy.id == request.strategy_id))
    strategy = result.scalars().first()

    if not strategy:
        raise HTTPException(
            status_code=404, detail=f"Strategy not found: {request.strategy_id}"
        )

    if not strategy.is_active:
        raise HTTPException(status_code=400, detail="Strategy is not active")

    # 2) Initial balance validation
    if request.initial_balance <= 0:
        raise HTTPException(
            status_code=400, detail="initial_balance must be greater than 0"
        )

    # 3) CSV path validation (ì˜µì…˜)
    if request.csv_path:
        validate_csv_path(request.csv_path)

    # 4) ì „ëµ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    import json

    strategy_params = json.loads(strategy.params) if strategy.params else {}
    strategy_type = strategy_params.get("type", "openclose")
    symbol = strategy_params.get("symbol", "BTCUSDT")
    timeframe = strategy_params.get("timeframe", "1h")

    # 5) DBì— pending ìƒíƒœë¡œ ë¨¼ì € ì €ì¥ (user_id í¬í•¨)
    backtest_result = BacktestResult(
        user_id=user_id,
        pair=symbol,
        timeframe=timeframe,
        initial_balance=request.initial_balance,
        final_balance=0.0,
        metrics="{}",
        equity_curve="[]",
        params=str(request.dict()),
        status="queued",
        created_at=datetime.utcnow(),
    )
    session.add(backtest_result)
    session.commit()
    session.refresh(backtest_result)

    result_id = backtest_result.id

    # 6) ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì €ì— ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘ ê¸°ë¡
    resource_manager.start_backtest(user_id, result_id)

    # 7) ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ìš© íŒŒë¼ë¯¸í„° ì¤€ë¹„
    task_params = {
        "strategy_id": request.strategy_id,
        "strategy_code": strategy_type,  # Use type from params, not DB code
        "strategy_params": strategy_params,
        "initial_balance": request.initial_balance,
        "start_date": request.start_date,
        "end_date": request.end_date,
        "csv_path": request.csv_path,
        "symbol": symbol,
        "timeframe": timeframe,
    }

    # 8) ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì¶”ê°€
    background_tasks.add_task(
        _run_backtest_background,
        result_id=result_id,
        request_dict=task_params,
        user_id=user_id,
    )

    # 4) ì¦‰ì‹œ ì‘ë‹µ
    response = {
        "status": "queued",
        "result_id": result_id,
        "final_balance": 0.0,
        "metrics": {},
    }
    return response


@router.get("/cache/info")
async def get_cache_info():
    """
    ë°±í…ŒìŠ¤íŠ¸ ìºì‹œ ì •ë³´ ì¡°íšŒ

    ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¬ë³¼, íƒ€ì„í”„ë ˆì„, ë°ì´í„° ë²”ìœ„ ë°˜í™˜

    Returns:
        - mode: í˜„ì¬ ë°ì´í„° ëª¨ë“œ (offline/online)
        - cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        - total_files: ì´ ìºì‹œ íŒŒì¼ ìˆ˜
        - available_data: ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ëª©ë¡
    """
    from ..services.candle_cache import get_candle_cache

    cache_manager = get_candle_cache()
    info = cache_manager.get_cache_info()

    # í”„ë¡ íŠ¸ì—”ë“œìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    available_data = []

    caches = info.get("caches", {})
    for cache_key, meta in caches.items():
        if isinstance(meta, dict) and "start" in meta:
            try:
                start_ts = meta.get("start", 0)
                end_ts = meta.get("end", 0)

                # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë°€ë¦¬ì´ˆì¸ ê²½ìš° ì´ˆë¡œ ë³€í™˜
                if start_ts > 1e12:
                    start_ts = start_ts / 1000
                if end_ts > 1e12:
                    end_ts = end_ts / 1000

                available_data.append(
                    {
                        "symbol": meta.get("symbol", cache_key.split("_")[0]),
                        "timeframe": meta.get(
                            "timeframe",
                            cache_key.split("_")[1] if "_" in cache_key else "1h",
                        ),
                        "candle_count": meta.get("count", 0),
                        "start_date": datetime.fromtimestamp(start_ts).strftime(
                            "%Y-%m-%d"
                        )
                        if start_ts
                        else None,
                        "end_date": datetime.fromtimestamp(end_ts).strftime("%Y-%m-%d")
                        if end_ts
                        else None,
                        "size_mb": round(meta.get("size_mb", 0), 2),
                        "updated_at": meta.get("updated_at"),
                    }
                )
            except Exception as e:
                # ë©”íƒ€ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜ ì‹œ ìŠ¤í‚µ
                continue

    return {
        "mode": BacktestConfig.DATA_MODE,
        "cache_only": BacktestConfig.CACHE_ONLY,
        "cache_dir": str(BacktestConfig.CACHE_DIR),
        "total_files": info.get("total_files", 0),
        "available_data": sorted(
            available_data, key=lambda x: (x["symbol"], x["timeframe"])
        ),
    }


@router.get("/cache/symbols")
async def get_available_symbols():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¬ë³¼ ëª©ë¡ ì¡°íšŒ (ê°„ë‹¨ ë²„ì „)

    Returns:
        - symbols: ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸
        - timeframes: ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ì„í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
    """
    from ..services.candle_cache import get_candle_cache

    cache_manager = get_candle_cache()
    info = cache_manager.get_cache_info()

    symbols = set()
    timeframes = set()

    caches = info.get("caches", {})
    for cache_key in caches.keys():
        parts = cache_key.replace(".csv", "").split("_")
        if len(parts) >= 2:
            symbols.add(parts[0])
            timeframes.add(parts[1])

    return {
        "symbols": sorted(list(symbols)),
        "timeframes": sorted(
            list(timeframes),
            key=lambda x: (
                0 if x.endswith("m") else 1 if x.endswith("h") else 2,
                int(x[:-1]) if x[:-1].isdigit() else 0,
            ),
        ),
    }
