"""
ìº”ë“¤ ë°ì´í„° ìºì‹± ì‹œìŠ¤í…œ

ë‹¤ì¤‘ ì‚¬ìš©ì ë°±í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê³µìš© ìº”ë“¤ ë°ì´í„° ìºì‹œ.
Bitget API Rate Limit ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.

ê¸°ëŠ¥:
1. ê³µìš© ìºì‹œ: ëª¨ë“  ì‚¬ìš©ìê°€ ë™ì¼í•œ ìº”ë“¤ ë°ì´í„° ê³µìœ 
2. ìŠ¤ë§ˆíŠ¸ ê°±ì‹ : ì—†ëŠ” ë°ì´í„°ë§Œ APIë¡œ ê°€ì ¸ì˜´
3. Rate Limit í: ë™ì‹œ ìš”ì²­ ìˆœì°¨ ì²˜ë¦¬
4. íŒŒì¼ ê¸°ë°˜ ì˜êµ¬ ì €ì¥: ì„œë²„ ì¬ì‹œì‘ í›„ì—ë„ ìœ ì§€
"""

import csv
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import json
import time

logger = logging.getLogger(__name__)


class CandleCacheManager:
    """
    ê³µìš© ìº”ë“¤ ë°ì´í„° ìºì‹œ ë§¤ë‹ˆì €

    ëª¨ë“  ì‚¬ìš©ìì˜ ë°±í…ŒìŠ¤íŠ¸ ìš”ì²­ì„ ìœ„í•œ ì¤‘ì•™ ìºì‹œ ê´€ë¦¬.
    """

    # ì§€ì›í•˜ëŠ” ì‹¬ë³¼ê³¼ íƒ€ì„í”„ë ˆì„
    SUPPORTED_SYMBOLS = ["BTCUSDT", "ETHUSDT", "SOLUSDT", "XRPUSDT", "DOGEUSDT"]
    SUPPORTED_TIMEFRAMES = ["1m", "5m", "15m", "30m", "1h", "4h", "1D"]

    # íƒ€ì„í”„ë ˆì„ë³„ ìº”ë“¤ ê°„ê²© (ë°€ë¦¬ì´ˆ)
    TIMEFRAME_MS = {
        "1m": 60 * 1000,
        "5m": 5 * 60 * 1000,
        "15m": 15 * 60 * 1000,
        "30m": 30 * 60 * 1000,
        "1h": 60 * 60 * 1000,
        "4h": 4 * 60 * 60 * 1000,
        "1D": 24 * 60 * 60 * 1000,
    }

    def __init__(self, cache_dir: Optional[str] = None):
        """
        ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”

        Args:
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        """
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            # ê¸°ë³¸: backend/candle_cache/
            self.cache_dir = Path(__file__).parent.parent.parent / "candle_cache"

        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ë©”ëª¨ë¦¬ ìºì‹œ (ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°)
        self._memory_cache: Dict[str, List[Dict]] = {}
        self._memory_cache_timestamps: Dict[str, float] = {}
        self._memory_cache_max_age = 300  # 5ë¶„

        # Rate Limit ê´€ë¦¬
        self._api_queue = asyncio.Queue()
        self._rate_limit_lock = asyncio.Lock()
        self._last_api_call = 0
        self._min_api_interval = 2.0  # 2ì´ˆ ê°„ê²© (Rate Limit ì•ˆì „)

        # ìºì‹œ ë©”íƒ€ë°ì´í„°
        self._metadata_file = self.cache_dir / "cache_metadata.json"
        self._metadata = self._load_metadata()

        logger.info(f"ğŸ“¦ CandleCacheManager initialized: {self.cache_dir}")

    def _load_metadata(self) -> Dict:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self._metadata_file.exists():
            try:
                with open(self._metadata_file, "r") as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load cache metadata: {e}")
        return {"caches": {}, "last_update": None}

    def _save_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            self._metadata["last_update"] = datetime.now().isoformat()
            with open(self._metadata_file, "w") as f:
                json.dump(self._metadata, f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save cache metadata: {e}")

    def _get_cache_key(self, symbol: str, timeframe: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"{symbol}_{timeframe}"

    def _get_cache_file(self, symbol: str, timeframe: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
        return self.cache_dir / f"{symbol}_{timeframe}.csv"

    async def get_candles(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
        cache_only: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        ìº”ë“¤ ë°ì´í„° ì¡°íšŒ (ìºì‹œ ìš°ì„ )

        1. ìºì‹œ í™•ì¸ â†’ ìˆìœ¼ë©´ ë°˜í™˜
        2. ìºì‹œ ì—†ê±°ë‚˜ ë¶€ì¡± â†’ cache_only=Trueë©´ ìºì‹œë§Œ ë°˜í™˜, ì•„ë‹ˆë©´ API í˜¸ì¶œ

        Args:
            symbol: ê±°ë˜ìŒ (ì˜ˆ: BTCUSDT)
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: 1h)
            start_date: ì‹œì‘ì¼ (YYYY-MM-DD)
            end_date: ì¢…ë£Œì¼ (YYYY-MM-DD)
            cache_only: Trueë©´ API í˜¸ì¶œ ì—†ì´ ìºì‹œ ë°ì´í„°ë§Œ ë°˜í™˜ (Rate Limit ë°©ì§€)

        Returns:
            ìº”ë“¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        symbol = symbol.upper().replace("/", "")
        cache_key = self._get_cache_key(symbol, timeframe)

        logger.info(
            f"ğŸ“Š Requesting candles: {symbol} {timeframe} ({start_date} ~ {end_date})"
        )

        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        memory_candles = self._get_from_memory_cache(cache_key, start_date, end_date)
        if memory_candles:
            logger.info(f"   âœ… Memory cache hit: {len(memory_candles)} candles")
            return memory_candles

        # 2. íŒŒì¼ ìºì‹œ í™•ì¸
        file_candles = self._get_from_file_cache(
            symbol, timeframe, start_date, end_date
        )

        # 3. í•„ìš”í•œ ê¸°ê°„ ê³„ì‚°
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59
        )

        if file_candles:
            # ìºì‹œì— ìˆëŠ” ê¸°ê°„ í™•ì¸
            cached_start = min(c["timestamp"] for c in file_candles)
            cached_end = max(c["timestamp"] for c in file_candles)

            start_ts = int(start_dt.timestamp() * 1000)
            end_ts = int(end_dt.timestamp() * 1000)

            # í•„ìš”í•œ ê¸°ê°„ì´ ìºì‹œ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
            if cached_start <= start_ts and cached_end >= end_ts:
                # ìºì‹œ ë²”ìœ„ ë‚´ â†’ í•„í„°ë§ í›„ ë°˜í™˜
                result = [
                    c for c in file_candles if start_ts <= c["timestamp"] <= end_ts
                ]
                logger.info(f"   âœ… File cache hit: {len(result)} candles")
                self._update_memory_cache(cache_key, result)
                return result

            # ë¶€ë¶„ ìºì‹œ â†’ ìºì‹œ ì „ìš© ëª¨ë“œë©´ ìºì‹œë§Œ ë°˜í™˜
            if cache_only:
                result = [
                    c for c in file_candles if start_ts <= c["timestamp"] <= end_ts
                ]
                if result:
                    logger.info(
                        f"   âœ… Cache only mode: {len(result)} candles (may be partial)"
                    )
                    self._update_memory_cache(cache_key, result)
                    return result
                else:
                    logger.warning(f"   âš ï¸ Cache only mode: no data in requested range")
                    return file_candles  # ì „ì²´ ìºì‹œ ë°˜í™˜

            # ë¶€ë¶„ ìºì‹œ â†’ ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ API í˜¸ì¶œ
            missing_ranges = self._calculate_missing_ranges(
                file_candles, start_ts, end_ts, timeframe
            )

            if missing_ranges:
                logger.info(
                    f"   âš ï¸ Partial cache, fetching {len(missing_ranges)} missing ranges"
                )
                new_candles = await self._fetch_missing_ranges(
                    symbol, timeframe, missing_ranges
                )
                # ê¸°ì¡´ ìºì‹œì™€ í•©ì¹˜ê¸°
                all_candles = file_candles + new_candles
                all_candles = self._deduplicate_candles(all_candles)
                self._save_to_file_cache(symbol, timeframe, all_candles)

                result = [
                    c for c in all_candles if start_ts <= c["timestamp"] <= end_ts
                ]
                self._update_memory_cache(cache_key, result)
                return result

        # 4. ìºì‹œ ì—†ìŒ
        if cache_only:
            logger.warning(
                f"   âš ï¸ Cache only mode: no cache available for {symbol} {timeframe}"
            )
            return []

        logger.info(f"   ğŸŒ No cache, fetching from Bitget API...")
        candles = await self._fetch_from_api(symbol, timeframe, start_date, end_date)

        if candles:
            # íŒŒì¼ ìºì‹œì— ì €ì¥
            self._save_to_file_cache(symbol, timeframe, candles)
            self._update_memory_cache(cache_key, candles)

        return candles

    def _get_from_memory_cache(
        self, cache_key: str, start_date: str, end_date: str
    ) -> Optional[List[Dict]]:
        """ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¡°íšŒ"""
        if cache_key not in self._memory_cache:
            return None

        # ìºì‹œ ë§Œë£Œ í™•ì¸
        cache_time = self._memory_cache_timestamps.get(cache_key, 0)
        if time.time() - cache_time > self._memory_cache_max_age:
            del self._memory_cache[cache_key]
            del self._memory_cache_timestamps[cache_key]
            return None

        candles = self._memory_cache[cache_key]

        # ìš”ì²­ ê¸°ê°„ í•„í„°ë§
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(
            hour=23, minute=59, second=59
        )
        start_ts = int(start_dt.timestamp() * 1000)
        end_ts = int(end_dt.timestamp() * 1000)

        return [c for c in candles if start_ts <= c["timestamp"] <= end_ts]

    def _update_memory_cache(self, cache_key: str, candles: List[Dict]):
        """ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸"""
        self._memory_cache[cache_key] = candles
        self._memory_cache_timestamps[cache_key] = time.time()

    def _get_from_file_cache(
        self, symbol: str, timeframe: str, start_date: str, end_date: str
    ) -> List[Dict]:
        """íŒŒì¼ ìºì‹œì—ì„œ ì¡°íšŒ"""
        cache_file = self._get_cache_file(symbol, timeframe)

        if not cache_file.exists():
            return []

        try:
            candles = []
            with open(cache_file, "r", newline="") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    candles.append(
                        {
                            "timestamp": int(row["timestamp"]),
                            "open": float(row["open"]),
                            "high": float(row["high"]),
                            "low": float(row["low"]),
                            "close": float(row["close"]),
                            "volume": float(row["volume"]),
                        }
                    )

            logger.debug("Loaded %d candles from %s", len(candles), cache_file)
            return candles

        except Exception as e:
            logger.error(f"Failed to read cache file {cache_file}: {e}")
            return []

    def _save_to_file_cache(self, symbol: str, timeframe: str, candles: List[Dict]):
        """íŒŒì¼ ìºì‹œì— ì €ì¥"""
        if not candles:
            return

        cache_file = self._get_cache_file(symbol, timeframe)

        try:
            # ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
            candles = sorted(candles, key=lambda x: x["timestamp"])

            with open(cache_file, "w", newline="") as f:
                writer = csv.DictWriter(
                    f,
                    fieldnames=["timestamp", "open", "high", "low", "close", "volume"],
                )
                writer.writeheader()
                writer.writerows(candles)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            cache_key = self._get_cache_key(symbol, timeframe)
            self._metadata["caches"][cache_key] = {
                "symbol": symbol,
                "timeframe": timeframe,
                "count": len(candles),
                "start": min(c["timestamp"] for c in candles),
                "end": max(c["timestamp"] for c in candles),
                "updated_at": datetime.now().isoformat(),
            }
            self._save_metadata()

            logger.info(f"   ğŸ’¾ Saved {len(candles)} candles to {cache_file.name}")

        except Exception as e:
            logger.error(f"Failed to save cache file {cache_file}: {e}")

    def _calculate_missing_ranges(
        self,
        cached_candles: List[Dict],
        start_ts: int,
        end_ts: int,
        timeframe: str,
    ) -> List[Tuple[int, int]]:
        """ìºì‹œì—ì„œ ëˆ„ë½ëœ ê¸°ê°„ ê³„ì‚°"""
        if not cached_candles:
            return [(start_ts, end_ts)]

        cached_start = min(c["timestamp"] for c in cached_candles)
        cached_end = max(c["timestamp"] for c in cached_candles)

        missing = []

        # ì‹œì‘ ì „ ëˆ„ë½
        if start_ts < cached_start:
            missing.append((start_ts, cached_start - 1))

        # ë í›„ ëˆ„ë½
        if end_ts > cached_end:
            missing.append((cached_end + 1, end_ts))

        return missing

    async def _fetch_missing_ranges(
        self,
        symbol: str,
        timeframe: str,
        missing_ranges: List[Tuple[int, int]],
    ) -> List[Dict]:
        """ëˆ„ë½ëœ ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ APIì—ì„œ ê°€ì ¸ì˜´"""
        all_candles = []

        for start_ts, end_ts in missing_ranges:
            start_date = datetime.fromtimestamp(start_ts / 1000).strftime("%Y-%m-%d")
            end_date = datetime.fromtimestamp(end_ts / 1000).strftime("%Y-%m-%d")

            candles = await self._fetch_from_api(
                symbol, timeframe, start_date, end_date
            )
            all_candles.extend(candles)

        return all_candles

    def _deduplicate_candles(self, candles: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ìº”ë“¤ ì œê±°"""
        seen = set()
        unique = []
        for c in candles:
            if c["timestamp"] not in seen:
                seen.add(c["timestamp"])
                unique.append(c)
        return sorted(unique, key=lambda x: x["timestamp"])

    async def _fetch_from_api(
        self,
        symbol: str,
        timeframe: str,
        start_date: str,
        end_date: str,
    ) -> List[Dict]:
        """
        Bitget APIì—ì„œ ìº”ë“¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Rate Limit ê´€ë¦¬)
        """
        from .bitget_rest import BitgetRestClient

        async with self._rate_limit_lock:
            # Rate Limit ëŒ€ê¸°
            elapsed = time.time() - self._last_api_call
            if elapsed < self._min_api_interval:
                await asyncio.sleep(self._min_api_interval - elapsed)

            try:
                client = BitgetRestClient()
                candles = await client.get_all_historical_candles(
                    symbol=symbol,
                    interval=timeframe,
                    start_time=start_date,
                    end_time=end_date,
                )
                self._last_api_call = time.time()

                logger.info(f"   ğŸŒ Fetched {len(candles)} candles from Bitget API")
                return candles

            except Exception as e:
                logger.error(f"Failed to fetch from Bitget API: {e}")
                raise

    def get_cache_info(self) -> Dict[str, Any]:
        """ìºì‹œ ì •ë³´ ì¡°íšŒ"""
        cache_files = list(self.cache_dir.glob("*.csv"))

        info = {
            "cache_dir": str(self.cache_dir),
            "total_files": len(cache_files),
            "caches": {},
        }

        for cache_file in cache_files:
            name = cache_file.stem
            stat = cache_file.stat()
            info["caches"][name] = {
                "size_mb": round(stat.st_size / 1024 / 1024, 2),
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            }

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for key, meta in self._metadata.get("caches", {}).items():
            if key in info["caches"]:
                info["caches"][key].update(meta)

        return info

    async def preload_popular_symbols(self):
        """
        ì¸ê¸° ì‹¬ë³¼ì˜ ìµœê·¼ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ

        ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œí•˜ì—¬ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë¥¼ ìºì‹±
        """
        logger.info("ğŸ”„ Preloading popular symbol data...")

        # ìµœê·¼ 1ë…„ ë°ì´í„° í”„ë¦¬ë¡œë“œ
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")

        # ê°€ì¥ ì¸ê¸° ìˆëŠ” ì‹¬ë³¼ê³¼ íƒ€ì„í”„ë ˆì„
        popular = [
            ("BTCUSDT", "1h"),
            ("ETHUSDT", "1h"),
            ("BTCUSDT", "4h"),
            ("ETHUSDT", "4h"),
        ]

        for symbol, timeframe in popular:
            try:
                await self.get_candles(symbol, timeframe, start_date, end_date)
                logger.info(f"   âœ… Preloaded {symbol} {timeframe}")
            except Exception as e:
                logger.warning(f"   âš ï¸ Failed to preload {symbol} {timeframe}: {e}")

            # Rate Limit ë°©ì§€
            await asyncio.sleep(1)

        logger.info("âœ… Preloading complete")

    def clear_cache(
        self, symbol: Optional[str] = None, timeframe: Optional[str] = None
    ):
        """
        ìºì‹œ ì‚­ì œ

        Args:
            symbol: ì‹¬ë³¼ (Noneì´ë©´ ì „ì²´)
            timeframe: íƒ€ì„í”„ë ˆì„ (Noneì´ë©´ ì „ì²´)
        """
        if symbol and timeframe:
            # íŠ¹ì • ìºì‹œë§Œ ì‚­ì œ
            cache_file = self._get_cache_file(symbol, timeframe)
            if cache_file.exists():
                cache_file.unlink()
                logger.info(f"ğŸ—‘ï¸ Deleted cache: {cache_file.name}")

            cache_key = self._get_cache_key(symbol, timeframe)
            if cache_key in self._memory_cache:
                del self._memory_cache[cache_key]
            if cache_key in self._metadata["caches"]:
                del self._metadata["caches"][cache_key]
                self._save_metadata()
        else:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            for cache_file in self.cache_dir.glob("*.csv"):
                cache_file.unlink()

            self._memory_cache.clear()
            self._memory_cache_timestamps.clear()
            self._metadata["caches"] = {}
            self._save_metadata()

            logger.info("ğŸ—‘ï¸ Cleared all cache")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_cache_manager: Optional[CandleCacheManager] = None


def get_candle_cache() -> CandleCacheManager:
    """ìº”ë“¤ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = CandleCacheManager()
    return _cache_manager
